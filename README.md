# ğŸ“¢ Toxic Comment Classification using DistilBERT

This project develops a **multi-label toxic comment classifier** using the **DistilBERT** architecture. It integrates **Optuna** for hyperparameter tuning, **stratified k-fold validation**, and **mixed-precision training** to enhance efficiency and performance.

---
## ğŸ§ About
Online discussions often contain harmful content, making automated moderation crucial. This project utilizes **natural language processing (NLP)** to classify comments into multiple toxicity categories. By leveraging **DistilBERT** as the feature extractor and a custom classifier head, the model achieves high accuracy in detecting toxic content.

---
## âœ¨ Key Features
âœ… **Advanced text preprocessing** (regex-based cleaning, contractions handling)  
âœ… **Multilabel stratified data splitting** for balanced training  
âœ… **Custom classifier head** with layer normalization  
âœ… **Hyperparameter optimization** using Optuna  
âœ… **Mixed-precision training** for computational efficiency  
âœ… **Model checkpointing** and metadata storage for reproducibility  

---
## âš™ï¸ Installation
1ï¸âƒ£ **Clone the repository** from GitHub:
```bash
git clone <https://github.com/Jaejun02/ToxicComment.git>
cd ToxicComment
```
2ï¸âƒ£ Install dependencies through **Conda environment**:
```bash
conda env create -f environment.yml
conda activate toxiccs
```
3ï¸âƒ£ **Combine the model files** by running:
```bash
cat ./saved_model/model.safetensors_* > ./saved_model/model.safetensors
```
4ï¸âƒ£ **Download your Kaggle API key** by going to Kaggle, then going to your account settings and clicking **Create New API Token**.

5ï¸âƒ£ **Set up your Kaggle API key** by running:
```bash
mkdir -p ~/.kaggle
mv /path/to/your/kaggle.json ~/.kaggle/kaggle.json
chmod 600 ~/.kaggle/kaggle.json
```

6ï¸âƒ£ **Open the Jupyter Notebook** (`toxic_comment_classifier.ipynb`) in JupyterLab or another compatible environment.

7ï¸âƒ£ **Execute the cells** in order to preprocess data, train the model, and evaluate results.


---
## ğŸš€ Usage
â˜‘ï¸ **Hyperparameter Tuning**
Set `HP_TUNING = True` in `Config` class and run `toxic_comment_classifier.ipynb`.

â˜‘ï¸ **Further Tuning and Final Training**
Set `FT_TUNING = True` and `FULL_TRAINING = True` in `Config` class and rurn `toxic_comment_classifier.ipynb`.


---
## ğŸ“Š Model Evaluation
Best test F1 Score: 0.947.

---
## ğŸ“œ License
This project is licensed under the **MIT License**.

---
ğŸ“Œ **Author:** Jaejun Shim  
ğŸ“† **Date:** December 23, 2024