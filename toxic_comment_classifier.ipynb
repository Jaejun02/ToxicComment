{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Toxic Comment Classification using DistilBERT with Hyperparameter Tuning\n",
    "\n",
    "This script trains a multi-label toxic comment classifier using the DistilBERT architecture with Optuna for hyperparameter optimization. It implements stratified k-fold validation, mixed-precision training, and gradient accumulation.\n",
    "\n",
    "### Key Features\n",
    "- Text preprocessing with regex patterns and contractions handling  \n",
    "- Multilabel stratified data splitting  \n",
    "- Custom classifier head with layer normalization  \n",
    "- Optuna integration for hyperparameter search  \n",
    "- Mixed-precision training with gradient accumulation  \n",
    "- Model checkpointing and metadata saving  \n",
    "\n",
    "**Author:** Jaejun Shim \n",
    "\n",
    "**Date:** 2024-12-23\n",
    "\n",
    "**License:** MIT  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import time\n",
    "import joblib\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import optuna\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.optim import AdamW\n",
    "from torch.amp import autocast\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "from iterstrat.ml_stratifiers import MultilabelStratifiedShuffleSplit\n",
    "import shutil\n",
    "import zipfile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    \"\"\"Global configuration parameters.\"\"\"\n",
    "    RANDOM_SEED = 42\n",
    "    BASE_MODEL = \"distilbert-base-uncased\"\n",
    "    DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    DATA_PATH = \"./data\"\n",
    "    MODEL_SAVE_PATH = \"./saved_model\"\n",
    "    BEST_PARAMS_PATH = \"./best_params.pkl\"\n",
    "    \n",
    "    # Training Flags\n",
    "    HP_TUNING = False\n",
    "    FT_TUNING = False\n",
    "    FULL_TRAINING = True\n",
    "\n",
    "    # Optimization Parameters\n",
    "    ACCUMULATION_STEPS = 4\n",
    "    MAX_EPOCHS = 10\n",
    "    PATIENCE = 2\n",
    "torch.manual_seed(Config.RANDOM_SEED)\n",
    "np.random.seed(Config.RANDOM_SEED)\n",
    "torch.backends.cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Downloading Files From Kaggle\n",
    "!kaggle competitions download -c jigsaw-toxic-comment-classification-challenge -q\n",
    "with zipfile.ZipFile(\"jigsaw-toxic-comment-classification-challenge.zip\", \"r\") as zip_ref:\n",
    "    zip_ref.extractall(Config.DATA_PATH)\n",
    "with zipfile.ZipFile(\"data/train.csv.zip\", \"r\") as zip_ref:\n",
    "    zip_ref.extractall(Config.DATA_PATH)\n",
    "for file in ['jigsaw-toxic-comment-classification-challenge.zip', './data/sample_submission.csv.zip', './data/test_labels.csv.zip',\n",
    "             './data/test.csv.zip', './data/train.csv.zip']:\n",
    "    if os.path.exists(file):\n",
    "        os.remove(file)\n",
    "    else:\n",
    "        raise FileNotFoundError(f\"File {file} not found!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Clean and normalize text data using regex patterns and contractions handling.\n",
    "    \n",
    "    Args:\n",
    "        text: Raw input text to be cleaned.\n",
    "        \n",
    "    Returns:\n",
    "        str: Cleaned and normalized text.\n",
    "    \"\"\"\n",
    "    # Remove special characters\n",
    "    text = re.sub('[“”¨«»®´·º½¾¿¡§£₤‘’â€¢&]', \"\", text)\n",
    "    \n",
    "    # Remove Wikipedia artifacts\n",
    "    text = re.sub(r\"WP:[A-Z]+\", \"\", text, flags=re.IGNORECASE)\n",
    "    \n",
    "    # Replace sensitive information\n",
    "    text = re.sub(r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b', \n",
    "                 'emailaddress', text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r'https?://\\S+', 'websiteurl', text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r'\\(?\\d{3}\\)?[\\s-]?\\d{3}[\\s-]?\\d{4}', 'phonenumber', text)\n",
    "    \n",
    "    # Normalize whitespace and fix contractions\n",
    "    text = re.sub(r'\\s+', ' ', text).strip().lower()\n",
    "    return contractions.fix(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(file_name: str, subsample: bool = False) -> tuple:\n",
    "    \"\"\"\n",
    "    Load and preprocess dataset with optional subsampling.\n",
    "    \n",
    "    Args:\n",
    "        file_name: Name of the CSV file containing the data.\n",
    "        subsample: Whether to subsample the data for faster experimentation.\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (texts, labels) as numpy arrays.\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(os.path.join(Config.DATA_PATH, file_name))\n",
    "    texts = df['comment_text']\n",
    "    labels = df.iloc[:,2:].values.astype(np.float32)\n",
    "    \n",
    "    if subsample:\n",
    "        splitter = MultilabelStratifiedShuffleSplit(\n",
    "            n_splits=1, \n",
    "            test_size=0.6,\n",
    "            random_state=Config.RANDOM_SEED\n",
    "        )\n",
    "        for train_index, _ in splitter.split(texts, labels):\n",
    "            texts = texts[train_index]\n",
    "            labels = labels[train_index]\n",
    "    \n",
    "    return texts.apply(clean_text).values, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PyTorch Data Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ToxicCommentDataset(Dataset):\n",
    "    \"\"\"Custom dataset for toxic comment classification\"\"\"\n",
    "    \n",
    "    def __init__(self, texts: np.ndarray, labels: np.ndarray, \n",
    "                 tokenizer: AutoTokenizer, max_length: int):\n",
    "        \"\"\"\n",
    "        Initialize dataset.\n",
    "        \n",
    "        Args:\n",
    "            texts: Array of preprocessed text samples.\n",
    "            labels: Array of multi-hot encoded labels.\n",
    "            tokenizer: Pretrained tokenizer instance.\n",
    "            max_length: Maximum sequence length for truncation.\n",
    "        \"\"\"\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx: int) -> dict:\n",
    "        \"\"\"Return tokenized sample and labels.\"\"\"\n",
    "        text = self.texts[idx]\n",
    "        labels = self.labels[idx]\n",
    "        \n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].squeeze(0),\n",
    "            'attention_mask': encoding['attention_mask'].squeeze(0),\n",
    "            'labels': torch.tensor(labels, dtype=torch.float)\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_model(params: dict) -> AutoModelForSequenceClassification:\n",
    "    \"\"\"\n",
    "    Initialize DistilBERT model with custom classifier head.\n",
    "    \n",
    "    Args:\n",
    "        params: Dictionary of hyperparameters.\n",
    "        \n",
    "    Returns:\n",
    "        AutoModelForSequenceClassification: Configured model instance.\n",
    "    \"\"\"\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        Config.BASE_MODEL,\n",
    "        num_labels=6,  # Number of toxicity classes\n",
    "        dropout=params['dropout'],\n",
    "        attention_dropout=params['dropout'],\n",
    "    ).to(Config.DEVICE)\n",
    "    \n",
    "    # Freeze base layers except last two\n",
    "    for param in model.distilbert.parameters():\n",
    "        param.requires_grad = False\n",
    "    for layer in model.distilbert.transformer.layer[-2:]:\n",
    "        for param in layer.parameters():\n",
    "            param.requires_grad = True\n",
    "            \n",
    "    # Custom classifier with layer normalization\n",
    "    model.classifier = torch.nn.Sequential(\n",
    "        torch.nn.Dropout(params['dropout']),\n",
    "        torch.nn.Linear(model.config.hidden_size, params['hidden_units']),\n",
    "        torch.nn.ReLU(),\n",
    "        torch.nn.LayerNorm(params['hidden_units']),\n",
    "        torch.nn.Dropout(params['dropout']),\n",
    "        torch.nn.Linear(params['hidden_units'], 6)\n",
    "    ).to(Config.DEVICE)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model: AutoModelForSequenceClassification, \n",
    "                  dataloader: DataLoader) -> float:\n",
    "    \"\"\"\n",
    "    Evaluate model performance on validation set.\n",
    "    \n",
    "    Args:\n",
    "        model: Model to evaluate.\n",
    "        dataloader: Validation dataloader.\n",
    "        \n",
    "    Returns:\n",
    "        float: Weighted F1 score.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    all_preds, all_labels = [], []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            inputs = {k: v.to(Config.DEVICE) for k, v in batch.items() \n",
    "                     if k != 'labels'}\n",
    "            labels = batch['labels'].cpu().numpy()\n",
    "            \n",
    "            outputs = model(**inputs)\n",
    "            preds = torch.sigmoid(outputs.logits).cpu().numpy()\n",
    "            all_preds.extend(preds)\n",
    "            all_labels.extend(labels)\n",
    "    \n",
    "    threshold_preds = (np.array(all_preds) > 0.5).astype(int)\n",
    "    return f1_score(all_labels, threshold_preds, average='weighted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model: nn.Module, \n",
    "                loader: DataLoader, \n",
    "                optimizer: torch.optim.Optimizer,\n",
    "                scaler: torch.amp.GradScaler) -> float:\n",
    "    \"\"\"\n",
    "    Perform single epoch of training with gradient accumulation\n",
    "    \n",
    "    Args:\n",
    "        model: Model to train\n",
    "        loader: Training data loader\n",
    "        optimizer: Configured optimizer\n",
    "        scaler: Gradient scaler for mixed precision\n",
    "        \n",
    "    Returns:\n",
    "        float: Average training loss for the epoch\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    \n",
    "    for step, batch in enumerate(loader):\n",
    "        # Move data to device\n",
    "        inputs = {k: v.to(Config.DEVICE, non_blocking=True) \n",
    "                 for k, v in batch.items() if k != 'labels'}\n",
    "        labels = batch['labels'].to(Config.DEVICE, non_blocking=True)\n",
    "        \n",
    "        # Mixed precision forward pass\n",
    "        with torch.amp.autocast(device_type='cuda', dtype=torch.float16):\n",
    "            outputs = model(**inputs)\n",
    "            loss = torch.nn.BCEWithLogitsLoss()(outputs.logits, labels)\n",
    "            loss = loss / Config.ACCUMULATION_STEPS\n",
    "            \n",
    "        # Gradient accumulation\n",
    "        scaler.scale(loss).backward()\n",
    "        \n",
    "        # Weight update every accumulation_steps\n",
    "        if (step + 1) % Config.ACCUMULATION_STEPS == 0:\n",
    "            scaler.unscale_(optimizer)\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "        total_loss += loss.item() * Config.ACCUMULATION_STEPS\n",
    "        \n",
    "    return total_loss / len(loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_early_stopping_training(model: nn.Module,\n",
    "                               train_loader: DataLoader,\n",
    "                               val_loader: DataLoader,\n",
    "                               optimizer: torch.optim.Optimizer,\n",
    "                               max_epochs: int = Config.MAX_EPOCHS) -> dict:\n",
    "    \"\"\"\n",
    "    Complete training loop with early stopping.\n",
    "    \n",
    "    Args:\n",
    "        model: Initialized model.\n",
    "        train_loader: Training data loader.\n",
    "        val_loader: Validation data loader.\n",
    "        optimizer: Configured optimizer.\n",
    "        max_epochs: Maximum number of training epochs.\n",
    "        \n",
    "    Returns:\n",
    "        dict: Training results containing:\n",
    "            - best_f1: Best validation F1 score.\n",
    "            - best_epoch: Epoch number of best performance.\n",
    "            - model_weights: Best model state dict.\n",
    "    \"\"\"\n",
    "    scaler = torch.amp.GradScaler(enabled=True)\n",
    "    best_results = {\n",
    "        'best_f1': 0.0,\n",
    "        'best_epoch': 0,\n",
    "        'model_weights': None,\n",
    "        'patience_counter': 0\n",
    "    }\n",
    "    \n",
    "    for epoch in range(max_epochs):\n",
    "        # Training phase\n",
    "        train_loss = train_epoch(model, train_loader, optimizer, scaler)\n",
    "        \n",
    "        # Validation phase\n",
    "        current_f1 = evaluate_model(model, val_loader)\n",
    "        print(f\"Epoch {epoch+1}/{max_epochs} | \"\n",
    "             f\"Train Loss: {train_loss:.4f} | \"\n",
    "             f\"Val F1: {current_f1:.4f}\")\n",
    "        \n",
    "        # Early stopping check\n",
    "        if current_f1 > best_results['best_f1']:\n",
    "            best_results.update({\n",
    "                'best_f1': current_f1,\n",
    "                'best_epoch': epoch + 1,\n",
    "                'model_weights': model.state_dict().copy(),\n",
    "                'patience_counter': 0\n",
    "            })\n",
    "        else:\n",
    "            best_results['patience_counter'] += 1\n",
    "            if best_results['patience_counter'] >= Config.PATIENCE:\n",
    "                print(f\"Early stopping triggered at epoch {epoch+1}\")\n",
    "                break\n",
    "                \n",
    "    return best_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial: optuna.Trial) -> float:\n",
    "    \"\"\"Optuna objective function for hyperparameter optimization\"\"\"\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Load and split data\n",
    "    texts, labels = load_data(\"train.csv\", subsample=True)\n",
    "    train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
    "        texts, labels, \n",
    "        test_size=0.2, \n",
    "        random_state=Config.RANDOM_SEED + trial.number\n",
    "    )\n",
    "    \n",
    "    # Suggest hyperparameters\n",
    "    params = {\n",
    "        'max_length': trial.suggest_categorical('max_length', [128, 256]),\n",
    "        'lr': trial.suggest_float('lr', 1e-5, 5e-5, log=True),\n",
    "        'batch_size': trial.suggest_categorical('batch_size', [32, 64]),\n",
    "        'num_epochs': trial.suggest_int('num_epochs', 2, 3),\n",
    "        'dropout': trial.suggest_float('dropout', 0.1, 0.4),\n",
    "        'hidden_units': trial.suggest_categorical('hidden_units', [256, 512]),\n",
    "    }\n",
    "    \n",
    "    # Create data loaders\n",
    "    train_dataset = ToxicCommentDataset(train_texts, train_labels, \n",
    "                                       tokenizer, params['max_length'])\n",
    "    train_loader = DataLoader(train_dataset, \n",
    "                             batch_size=params['batch_size'], \n",
    "                             shuffle=True, \n",
    "                             num_workers=4)\n",
    "    \n",
    "    # Initialize model and optimizer\n",
    "    model = initialize_model(params)\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=params['lr'])\n",
    "    scaler = torch.amp.GradScaler(enabled=True)\n",
    "    \n",
    "    # Training loop\n",
    "    model.train()\n",
    "    for epoch in range(params['num_epochs']):\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        for step, batch in enumerate(train_loader):\n",
    "            inputs = {k: v.to(Config.DEVICE) for k, v in batch.items() \n",
    "                     if k != 'labels'}\n",
    "            labels = batch['labels'].to(Config.DEVICE)\n",
    "            \n",
    "            with torch.amp.autocast(device_type='cuda', dtype=torch.float16):\n",
    "                outputs = model(**inputs)\n",
    "                loss = torch.nn.BCEWithLogitsLoss()(outputs.logits, labels)\n",
    "                loss = loss / Config.ACCUMULATION_STEPS\n",
    "                \n",
    "            scaler.scale(loss).backward()\n",
    "            \n",
    "            if (step + 1) % Config.ACCUMULATION_STEPS == 0:\n",
    "                scaler.unscale_(optimizer)\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "                optimizer.zero_grad()\n",
    "    \n",
    "    # Validation and cleanup\n",
    "    val_dataset = ToxicCommentDataset(val_texts, val_labels, \n",
    "                                     tokenizer, params['max_length'])\n",
    "    val_loader = DataLoader(val_dataset, \n",
    "                           batch_size=params['batch_size']*2, \n",
    "                           shuffle=False)\n",
    "    \n",
    "    val_f1 = evaluate_model(model, val_loader)\n",
    "    del model, train_loader, val_loader\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    print(f\"Trial {trial.number} completed in {(time.time()-start_time)/60:.1f}m\")\n",
    "    return val_f1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Further Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def further_tune_model(best_params: dict):\n",
    "    \"\"\"\n",
    "    Given the best hyperparameters from Optuna study, further tuen the model to find the best epoch number.\n",
    "    \n",
    "    Args:\n",
    "        best_params: Optimized hyperparameters from Optuna study\n",
    "\n",
    "    Returns:\n",
    "        dict: Training results containing:\n",
    "            - best_f1: Best validation F1 score.\n",
    "            - best_epoch: Epoch number of best performance.\n",
    "            - model_weights: Best model weights.\n",
    "            - patience_counter: Number of epochs without improvement.\n",
    "    \"\"\"\n",
    "    # Data preparation\n",
    "    texts, labels = load_data(\"train.csv\")\n",
    "    splitter = MultilabelStratifiedShuffleSplit(n_splits=1, test_size=0.2)\n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(Config.BASE_MODEL)\n",
    "    for train_idx, val_idx in splitter.split(texts, labels):\n",
    "        train_dataset = ToxicCommentDataset(texts[train_idx], labels[train_idx],\n",
    "                                           tokenizer, best_params['max_length'])\n",
    "        val_dataset = ToxicCommentDataset(texts[val_idx], labels[val_idx],\n",
    "                                         tokenizer, best_params['max_length'])\n",
    "        \n",
    "    # Initialize training components\n",
    "    model = initialize_model(best_params)\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=best_params['lr'])\n",
    "    train_loader = DataLoader(train_dataset,\n",
    "                             batch_size=best_params['batch_size'],\n",
    "                             shuffle=True,\n",
    "                             num_workers=4)\n",
    "    val_loader = DataLoader(val_dataset,\n",
    "                           batch_size=best_params['batch_size']*2,\n",
    "                           shuffle=False)\n",
    "    \n",
    "    # Run training with early stopping\n",
    "    training_results = run_early_stopping_training(\n",
    "        model=model,\n",
    "        train_loader=train_loader,\n",
    "        val_loader=val_loader,\n",
    "        optimizer=optimizer\n",
    "    )\n",
    "    \n",
    "    return training_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main Execution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(Config.BASE_MODEL)\n",
    "\n",
    "# Hyperparameter Optimization\n",
    "if Config.HP_TUNING:\n",
    "    study = optuna.create_study(\n",
    "        direction=\"maximize\",\n",
    "        sampler=optuna.samplers.TPESampler(seed=Config.RANDOM_SEED),\n",
    "        storage=\"sqlite:///optuna.db\",\n",
    "        study_name=\"toxic_comment_study\",\n",
    "        load_if_exists=True\n",
    "    )\n",
    "    study.optimize(objective, n_trials=200)\n",
    "    joblib.dump(study.best_params, Config.BEST_PARAMS_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Further Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if Config.FT_TUNING:\n",
    "    best_params = joblib.load(Config.BEST_PARAMS_PATH)\n",
    "    training_results = further_tune_model(best_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Final Model Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if Config.FULL_TRAINING:\n",
    "    best_params = joblib.load(Config.BEST_PARAMS_PATH)\n",
    "    texts, labels = load_data(\"train.csv\")\n",
    "    \n",
    "    # Load full data\n",
    "    full_dataset = ToxicCommentDataset(texts, labels, tokenizer, best_params['max_length'])\n",
    "    \n",
    "    # Initialize model and training components\n",
    "    final_model = initialize_model(best_params)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(Config.BASE_MODEL)\n",
    "    optimizer = torch.optim.AdamW(final_model.parameters(), lr=best_params['lr'])\n",
    "    full_loader = DataLoader(full_dataset, \n",
    "                                batch_size=best_params['batch_size'],\n",
    "                                shuffle=True)\n",
    "    scaler = torch.amp.GradScaler(enabled=True)\n",
    "\n",
    "    # Final training loop\n",
    "    best_f1 = 0\n",
    "    patience_counter = 0\n",
    "    \n",
    "    final_model.train()\n",
    "    for epoch in range(training_results['best_epoch']):\n",
    "        optimizer.zero_grad()\n",
    "        for step, batch in enumerate(full_loader):\n",
    "            inputs = {k: v.to(device, non_blocking=True) for k, v in batch.items() if k != 'labels'}\n",
    "            labels = batch['labels'].to(device, non_blocking=True)\n",
    "\n",
    "            with autocast(device_type=Config.DEVICE, dtype=torch.float16):\n",
    "                outputs = final_model(**inputs)\n",
    "                loss = torch.nn.BCEWithLogitsLoss()(outputs.logits, labels)\n",
    "                loss = loss / Config.ACCUMULATION_STEPS\n",
    "            \n",
    "            scaler.scale(loss).backward()\n",
    "\n",
    "            if (step + 1) % Config.ACCUMULATION_STEPS == 0:\n",
    "                scaler.unscale_(optimizer)\n",
    "                torch.nn.utils.clip_grad_norm_(final_model.parameters(), 1.0)\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "                optimizer.zero_grad()\n",
    "    \n",
    "    # Save final model\n",
    "    final_model.save_pretrained(Config.MODEL_SAVE_PATH)\n",
    "    tokenizer.save_pretrained(Config.MODEL_SAVE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = './data/train.csv'\n",
    "if os.path.exists(file):\n",
    "    os.remove(file)\n",
    "else:\n",
    "    raise FileNotFoundError(f\"File {file} not found!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
